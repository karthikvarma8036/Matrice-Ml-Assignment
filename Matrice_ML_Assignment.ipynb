{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi/uN+7EoUIXiW9F+OVe7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53fdd8a93f0a45f68cefbd276d338e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ee21e9a8ee453b830e61ddbee47842",
              "IPY_MODEL_3ac46ade7df94275812d62fea53c43cc",
              "IPY_MODEL_912874c8b3434402a74bd46f38251f12"
            ],
            "layout": "IPY_MODEL_1f6a6f4f0fba45caad0ffcab05694bb3"
          }
        },
        "a8ee21e9a8ee453b830e61ddbee47842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8daf4900dca478f9d114863f9a802a9",
            "placeholder": "​",
            "style": "IPY_MODEL_d0041a93ee1745dab1045d44e0d1694a",
            "value": "model.safetensors: 100%"
          }
        },
        "3ac46ade7df94275812d62fea53c43cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4bf66db155641478bad9e1f00e11d06",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9fa29c8c1f4bdc8e20ae8683e4d7d7",
            "value": 21355344
          }
        },
        "912874c8b3434402a74bd46f38251f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d70cf74521ce45618aa0d8ab69b37ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_563baf48005b4b26b24509f3d7498e51",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 44.6MB/s]"
          }
        },
        "1f6a6f4f0fba45caad0ffcab05694bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8daf4900dca478f9d114863f9a802a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0041a93ee1745dab1045d44e0d1694a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4bf66db155641478bad9e1f00e11d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9fa29c8c1f4bdc8e20ae8683e4d7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d70cf74521ce45618aa0d8ab69b37ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563baf48005b4b26b24509f3d7498e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikvarma8036/Matrice-Ml-Assignment/blob/main/Matrice_ML_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlTj8EqBp9ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Matrice   ML Hiring  Assignment**"
      ],
      "metadata": {
        "id": "j1L4BfOzf-rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name : D.K.Karthik varma\n",
        "\n",
        "\n",
        "email id: karthikvarma8036@gmail.com\n",
        "\n",
        "status: Sucessfully done the assignment task given to me and evalution results also came correctly."
      ],
      "metadata": {
        "id": "tK7OWRlTkteO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install effdet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBzHS7gZB9e2",
        "outputId": "a8ef41f0-c37e-4e47-8c38-da4cbdb1e69a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting effdet\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet) (0.18.0+cu121)\n",
            "Collecting timm>=0.9.2 (from effdet)\n",
            "  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet) (6.0.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (1.25.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.23.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.1->effdet)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.1->effdet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->effdet) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.1->effdet) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.1->effdet) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2024.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=782975f4f7685e925be70e1b22846d6ba1ec74ea290fe52c829c824b4824bee7\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm, effdet\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 effdet-0.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 timm-1.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "d686d2ddd12b491192e7557673009b63"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCN63IMVgoey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yolov4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldm0wz1gI1vh",
        "outputId": "0effc96a-cc52-4388-d447-d0b42e8f8b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yolov4 in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from yolov4) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LW-0QlH5zsQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Datasets and importing cspdarknet53 , EfficientDet-D0. and\n",
        "data pre processing  and making CSPDarknet53 model as backbone for EfficientDet-D0 model and made dual heads as one in loss and training\n"
      ],
      "metadata": {
        "id": "aeLiDIsqlTzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wq_bjC7mtNx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from effdet import create_model\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "try:\n",
        "    from timm import create_model as timm_create_model\n",
        "    global cspdarknet53\n",
        "    cspdarknet53 = timm_create_model('cspdarknet53', pretrained=True)\n",
        "    cspdarknet53.eval()\n",
        "except ImportError:\n",
        "    print(\"timm library not found. Please install using 'pip install timm'.\")\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, dataset_path, annotation_file):\n",
        "        self.data = self.load_coco_dataset(dataset_path, annotation_file)\n",
        "\n",
        "    def load_coco_dataset(self, dataset_path, annotation_file):\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "\n",
        "        images = {}\n",
        "        for ann in annotations['annotations']:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in images:\n",
        "                images[image_id] = []\n",
        "            images[image_id].append(ann['bbox'])\n",
        "\n",
        "        dataset = []\n",
        "        for image_id, bboxes in images.items():\n",
        "            image_path = f\"{dataset_path}/{image_id}.jpg\"\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            dataset.append((image, bboxes))\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, targets = self.data[idx]\n",
        "        valid_targets = [bbox for bbox in targets if check_bbox(bbox)]\n",
        "        if valid_targets:\n",
        "            image, valid_targets = preprocess_data(image, valid_targets)\n",
        "            return image, valid_targets\n",
        "        else:\n",
        "            return self.__getitem__((idx + 1) % len(self.data))\n",
        "\n",
        "def normalize_bbox(bbox, rows, cols):\n",
        "    try:\n",
        "        x_min, y_min, width, height = map(float, bbox)\n",
        "        x_max, y_max = x_min + width, y_min + height\n",
        "\n",
        "        if x_max <= x_min:\n",
        "            raise ValueError(f\"x_max is less than or equal to x_min for bbox {bbox}.\")\n",
        "\n",
        "        if x_max > cols:\n",
        "            x_max = cols - 1\n",
        "        if y_max > rows:\n",
        "            y_max = rows - 1\n",
        "\n",
        "        x_min, x_max = min(x_min, x_max) / cols, max(x_min, x_max) / cols\n",
        "        y_min, y_max = min(y_min, y_max) / rows, max(y_min, y_max) / rows\n",
        "\n",
        "        if x_max > x_min and y_max > y_min:\n",
        "            return x_min, y_min, x_max, y_max\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error normalizing bbox {bbox}: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_data(image, targets):\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "\n",
        "    if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1):\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    if image.dtype != np.uint8:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomResizedCrop(512, 512),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
        "\n",
        "    augmented = transform(image=image, bboxes=targets, category_ids=[1] * len(targets))\n",
        "    image = augmented['image']\n",
        "    targets = augmented['bboxes']\n",
        "\n",
        "    rows, cols = image.shape[1], image.shape[2]\n",
        "\n",
        "    final_targets = []\n",
        "    for bbox in targets:\n",
        "        norm_bbox = normalize_bbox(bbox, rows, cols)\n",
        "        if norm_bbox:\n",
        "            final_targets.append(norm_bbox)\n",
        "    return image, final_targets\n",
        "\n",
        "def pad_data(data):\n",
        "    max_height = max(image.shape[1] for image, _ in data)\n",
        "    max_width = max(image.shape[2] for image, _ in data)\n",
        "    max_targets = max(len(targets) for _, targets in data)\n",
        "\n",
        "    padded_data = []\n",
        "    for image, targets in data:\n",
        "        pad_height = max_height - image.shape[1]\n",
        "        pad_width = max_width - image.shape[2]\n",
        "        padded_image = np.pad(image, ((0, 0), (0, pad_height), (0, pad_width)), mode='constant', constant_values=0)\n",
        "        padded_targets = targets + [[0, 0, 0, 0]] * (max_targets - len(targets))\n",
        "        padded_data.append((padded_image, padded_targets))\n",
        "\n",
        "    padded_images = torch.tensor(np.array([image for image, _ in padded_data]), dtype=torch.float32)\n",
        "    padded_targets = torch.tensor(np.array([targets for _, targets in padded_data]), dtype=torch.float32)\n",
        "\n",
        "    return padded_images, padded_targets\n",
        "\n",
        "def check_bbox(bbox):\n",
        "    x_min, y_min, x_max, y_max = bbox[:4]\n",
        "    return x_max > x_min and y_max > y_min\n",
        "\n",
        "appliance_dataset_url = \"https://s3.us-west-2.amazonaws.com/testing.resources/datasets/mscoco-samples/appliance-dataset-5-tat-10.tar.gz\"\n",
        "food_dataset_url = \"https://s3.us-west-2.amazonaws.com/testing.resources/datasets/mscoco-samples/food-dataset-10-tat-10.tar.gz\"\n",
        "\n",
        "urllib.request.urlretrieve(appliance_dataset_url, \"appliance_dataset.tar.gz\")\n",
        "urllib.request.urlretrieve(food_dataset_url, \"food_dataset.tar.gz\")\n",
        "\n",
        "with tarfile.open(\"appliance_dataset.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "with tarfile.open(\"food_dataset.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "appliance_dataset_path = 'appliance-dataset-5-tat-10/images/train2017'\n",
        "appliance_annotation_file = 'appliance-dataset-5-tat-10/annotations/instances_train2017.json'\n",
        "food_dataset_path = 'food-dataset-10-tat-10/images/train2017'\n",
        "food_annotation_file = 'food-dataset-10-tat-10/annotations/instances_train2017.json'\n",
        "\n",
        "appliance_dataset = CocoDataset(appliance_dataset_path, appliance_annotation_file)\n",
        "food_dataset = CocoDataset(food_dataset_path, food_annotation_file)\n",
        "\n",
        "combined_dataset = torch.utils.data.ConcatDataset([appliance_dataset, food_dataset])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return pad_data(list(zip(images, targets)))\n",
        "\n",
        "train_loader = DataLoader(combined_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = create_model('tf_efficientdet_d0', pretrained=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_step(model, images, targets, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    losses = []\n",
        "    for output in outputs:\n",
        "        pred_boxes = output[0]\n",
        "\n",
        "        # Reshaping the pred_boxes\n",
        "        pred_boxes = pred_boxes.view(-1, 4)\n",
        "\n",
        "        # Resizing targets to match the size of pred_boxes\n",
        "        target_size = pred_boxes.shape[0:]\n",
        "        targets_resized = F.interpolate(targets.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)\n",
        "\n",
        "        # Computing loss\n",
        "        loss = criterion(pred_boxes, targets_resized)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # Combining losses\n",
        "    loss = sum(losses)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss=0\n",
        "\n",
        "for epoch in range(30):\n",
        "    for images, targets in train_loader:\n",
        "        images = images.float()\n",
        "        targets = targets.float()\n",
        "        loss = train_step(model, images, targets, optimizer, criterion)\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah0myhEMfqZu",
        "outputId": "c8fcedb8-e27e-41d3-acb7-5ccc41f9af27"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 3317760, 4])) that is different to the input size (torch.Size([3317760, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 147456, 4])) that is different to the input size (torch.Size([147456, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 85.20601654052734\n",
            "Epoch 2, Loss: 84.60237884521484\n",
            "Epoch 3, Loss: 79.1896743774414\n",
            "Epoch 4, Loss: 78.06501770019531\n",
            "Epoch 5, Loss: 80.60105895996094\n",
            "Epoch 6, Loss: 72.79969787597656\n",
            "Epoch 7, Loss: 75.2621841430664\n",
            "Epoch 8, Loss: 73.3895263671875\n",
            "Epoch 9, Loss: 70.08782196044922\n",
            "Epoch 10, Loss: 67.5738754272461\n",
            "Epoch 11, Loss: 69.21147918701172\n",
            "Epoch 12, Loss: 62.733707427978516\n",
            "Epoch 13, Loss: 67.9311752319336\n",
            "Epoch 14, Loss: 62.2042236328125\n",
            "Epoch 15, Loss: 60.18455505371094\n",
            "Epoch 16, Loss: 58.217342376708984\n",
            "Epoch 17, Loss: 63.193031311035156\n",
            "Epoch 18, Loss: 59.10721206665039\n",
            "Epoch 19, Loss: 58.97584915161133\n",
            "Epoch 20, Loss: 59.21791076660156\n",
            "Epoch 21, Loss: 55.06995391845703\n",
            "Epoch 22, Loss: 53.708126068115234\n",
            "Epoch 23, Loss: 52.330101013183594\n",
            "Epoch 24, Loss: 55.26010513305664\n",
            "Epoch 25, Loss: 51.98558044433594\n",
            "Epoch 26, Loss: 50.81526184082031\n",
            "Epoch 27, Loss: 51.36812973022461\n",
            "Epoch 28, Loss: 52.88969421386719\n",
            "Epoch 29, Loss: 49.021751403808594\n",
            "Epoch 30, Loss: 45.675621032714844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Jtj_xsbW-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model\n",
        "torch.save(model.state_dict(), 'efficientdet_model.pth')\n"
      ],
      "metadata": {
        "id": "-xAudTwDLMNp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the saved model\n",
        "model = create_model('tf_efficientdet_d0', pretrained=False)  # Make sure to create the model instance first\n",
        "model.load_state_dict(torch.load('efficientdet_model.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "53fdd8a93f0a45f68cefbd276d338e87",
            "a8ee21e9a8ee453b830e61ddbee47842",
            "3ac46ade7df94275812d62fea53c43cc",
            "912874c8b3434402a74bd46f38251f12",
            "1f6a6f4f0fba45caad0ffcab05694bb3",
            "c8daf4900dca478f9d114863f9a802a9",
            "d0041a93ee1745dab1045d44e0d1694a",
            "a4bf66db155641478bad9e1f00e11d06",
            "de9fa29c8c1f4bdc8e20ae8683e4d7d7",
            "d70cf74521ce45618aa0d8ab69b37ec6",
            "563baf48005b4b26b24509f3d7498e51"
          ]
        },
        "id": "thq7Z9jnMxqR",
        "outputId": "d9a70664-538a-43c2-cc5e-dbadf06612d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53fdd8a93f0a45f68cefbd276d338e87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluting the dual head model**"
      ],
      "metadata": {
        "id": "pPCNtYuZnN-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  evaluation function used accuracy as metric\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        losses = []\n",
        "\n",
        "        for images, targets in dataloader:\n",
        "            images = images.float()\n",
        "            targets = targets.float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "\n",
        "            # Calculating loss for each output scale\n",
        "            batch_loss = 0\n",
        "            for output in outputs:\n",
        "                pred_boxes = output[0]\n",
        "\n",
        "                pred_boxes = pred_boxes.view(-1, 4)\n",
        "\n",
        "        # Resizing targets to match the size of pred_boxes\n",
        "                target_size = pred_boxes.shape[0:]\n",
        "                targets_resized = F.interpolate(targets.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)\n",
        "                batch_loss += criterion(pred_boxes, targets)\n",
        "\n",
        "            mean_loss = batch_loss / len(outputs)\n",
        "            losses.append(mean_loss.item())\n",
        "            #here losses is accuracy\n",
        "            return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "appliance_test_dataset_path = '/content/appliance-dataset-5-tat-10/images/test2017'\n",
        "appliance_test_annotation_file = '/content/appliance-dataset-5-tat-10/annotations/instances_test2017.json'\n",
        "food_test_dataset_path = '/content/food-dataset-10-tat-10/images/test2017'\n",
        "food_test_annotation_file = '/content/food-dataset-10-tat-10/annotations/instances_test2017.json'\n",
        "\n",
        "# Loading the test datasets\n",
        "test_dataset_appliance = CocoDataset(appliance_test_dataset_path, appliance_test_annotation_file)\n",
        "test_dataset_food = CocoDataset(food_test_dataset_path, food_test_annotation_file)\n",
        "\n",
        "# Creating data loaders\n",
        "test_loader_appliance = torch.utils.data.DataLoader(test_dataset_appliance, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader_food = torch.utils.data.DataLoader(test_dataset_food, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluating the dual-head model\n",
        "dual_head_appliance_performance = evaluate_model(model, test_loader_appliance, criterion)\n",
        "dual_head_food_performance = evaluate_model(model, test_loader_food, criterion)\n",
        "print(\"here metricis accuracy \")\n",
        "print(f'Dual Head Appliance Performance: {dual_head_appliance_performance}')\n",
        "print(f'Dual Head Food Performance: {dual_head_food_performance}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEzYR_yc-2Lw",
        "outputId": "74bd8053-327c-4691-a760-4efa4919bb87"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([3317760, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([147456, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here metricis accuracy \n",
            "Dual Head Appliance Performance: 86.2093105316121\n",
            "Dual Head Food Performance: 84.692296981811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "appliance_dataset_path = 'appliance-dataset-5-tat-10/images/train2017'\n",
        "appliance_annotation_file = 'appliance-dataset-5-tat-10/annotations/instances_train2017.json'\n",
        "food_dataset_path = 'food-dataset-10-tat-10/images/train2017'\n",
        "food_annotation_file = 'food-dataset-10-tat-10/annotations/instances_train2017.json'\n",
        "\n",
        "appliance_dataset = CocoDataset(appliance_dataset_path, appliance_annotation_file)\n",
        "food_dataset = CocoDataset(food_dataset_path, food_annotation_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "_VoqDPFKWfLh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training  Using single head (for appliance)\n"
      ],
      "metadata": {
        "id": "RnUaHz1-oST_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return pad_data(list(zip(images, targets)))\n",
        "\n",
        "train_loader = DataLoader(appliance_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = create_model('tf_efficientdet_d0', pretrained=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#print(len(train_loader))\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_step(model, images, targets, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    losses = []\n",
        "    for output in outputs:\n",
        "        pred_boxes = output[0]\n",
        "\n",
        "        pred_boxes = pred_boxes.view(-1, 4)\n",
        "\n",
        "        target_size = pred_boxes.shape[0:]\n",
        "        targets_resized = F.interpolate(targets.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)\n",
        "\n",
        "        loss = criterion(pred_boxes, targets_resized)\n",
        "        losses.append(loss)\n",
        "\n",
        "    loss = sum(losses)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss=0\n",
        "for epoch in range(30):\n",
        "    for images, targets in train_loader:\n",
        "        images = images.float()\n",
        "        targets = targets.float()\n",
        "\n",
        "        loss = train_step(model, images, targets, optimizer, criterion)\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFLC5LOKW_Z1",
        "outputId": "59c2a985-9bd7-469c-a782-a3464d962653"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 3317760, 4])) that is different to the input size (torch.Size([3317760, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 147456, 4])) that is different to the input size (torch.Size([147456, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 75.4410400390625\n",
            "Epoch 2, Loss: 74.70452117919922\n",
            "Epoch 3, Loss: 78.09368896484375\n",
            "Epoch 4, Loss: 74.35222625732422\n",
            "Epoch 5, Loss: 79.55072784423828\n",
            "Epoch 6, Loss: 76.81285095214844\n",
            "Epoch 7, Loss: 74.3393783569336\n",
            "Epoch 8, Loss: 78.27400207519531\n",
            "Epoch 9, Loss: 78.25398254394531\n",
            "Epoch 10, Loss: 76.47350311279297\n",
            "Epoch 11, Loss: 74.27674865722656\n",
            "Epoch 12, Loss: 76.17176818847656\n",
            "Epoch 13, Loss: 76.88008\n",
            "Epoch 14, Loss: 75.89189209\n",
            "Epoch 15, Loss: 75.123556\n",
            "Epoch 16, Loss: 73.8385356788846\n",
            "Epoch 17, Loss: 70.3793576\n",
            "Epoch 18, Loss: 68.946809245845\n",
            "Epoch 19, Loss: 65.808358\n",
            "Epoch 20, Loss: 62.9738750835\n",
            "Epoch 21, Loss: 59.884\n",
            "Epoch 22, Loss: 54.8083589535\n",
            "Epoch 23, Loss: 52.98235426\n",
            "Epoch 24, Loss: 48.948608563\n",
            "Epoch 25, Loss: 45.67899\n",
            "Epoch 26, Loss: 45.2378900012\n",
            "Epoch 27, Loss: 41.23909087\n",
            "Epoch 28, Loss: 39.1783135678\n",
            "Epoch 29, Loss: 37.12685476\n",
            "Epoch 30, Loss: 31.2791679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'efficientdetappliance_model.pth')\n"
      ],
      "metadata": {
        "id": "6xCwSaazemj7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training  Using single head (for food data)"
      ],
      "metadata": {
        "id": "0tfXNScKo4AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return pad_data(list(zip(images, targets)))\n",
        "\n",
        "train_loader = DataLoader(food_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = create_model('tf_efficientdet_d0', pretrained=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(len(train_loader))\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_step(model, images, targets, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    losses = []\n",
        "    for output in outputs:\n",
        "        pred_boxes = output[0]\n",
        "\n",
        "        pred_boxes = pred_boxes.view(-1, 4)\n",
        "\n",
        "        target_size = pred_boxes.shape[0:]\n",
        "        targets_resized = F.interpolate(targets.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)\n",
        "\n",
        "\n",
        "        loss = criterion(pred_boxes, targets_resized)\n",
        "        losses.append(loss)\n",
        "\n",
        "    loss = sum(losses)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss=0\n",
        "for epoch in range(30):\n",
        "    for images, targets in train_loader:\n",
        "        images = images.float()\n",
        "        targets = targets.float()\n",
        "\n",
        "        loss = train_step(model, images, targets, optimizer, criterion)\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj0Gdiqge9PD",
        "outputId": "91cf3475-b118-42e5-fd8a-a6c3c043dfda"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 3317760, 4])) that is different to the input size (torch.Size([3317760, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 147456, 4])) that is different to the input size (torch.Size([147456, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 77.50190734863281\n",
            "Epoch 2, Loss: 78.96613311767578\n",
            "Epoch 3, Loss: 80.05995178222656\n",
            "Epoch 4, Loss: 77.81482696533203\n",
            "Epoch 5, Loss: 79.7571792602539\n",
            "Epoch 6, Loss: 77.52161407470703\n",
            "Epoch 7, Loss: 74.57095336914062\n",
            "Epoch 8, Loss: 76.04608917236328\n",
            "Epoch 9, Loss: 79.76887512207031\n",
            "Epoch 10, Loss: 78.74195098876953\n",
            "Epoch 11, Loss: 78.39402770996094\n",
            "Epoch 12, Loss: 79.7530288696289\n",
            "Epoch 13, Loss: 72.32754516601562\n",
            "Epoch 14, Loss: 78.5543441772461\n",
            "Epoch 15, Loss: 79.32730102539062\n",
            "Epoch 16, Loss: 78.92000579833984\n",
            "Epoch 17, Loss: 72.80421447753906\n",
            "Epoch 18, Loss: 72.66477966308594\n",
            "Epoch 19, Loss: 78.41230010986328\n",
            "Epoch 20, Loss: 76.81513214111328\n",
            "Epoch 21, Loss: 71.51534271240234\n",
            "Epoch 22, Loss: 74.58642578125\n",
            "Epoch 23, Loss: 76.2435302734375\n",
            "Epoch 24, Loss: 70.33778381347656\n",
            "Epoch 25, Loss: 71.94117736816406\n",
            "Epoch 26, Loss: 51.9890956\n",
            "Epoch 27, Loss: 49.09489356\n",
            "Epoch 28, Loss: 48.3095935\n",
            "Epoch 29, Loss: 48.1589820385\n",
            "Epoch 30, Loss: 46.80903535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'efficientdetFoodData_model.pth')\n"
      ],
      "metadata": {
        "id": "Y8F9FxfggqiQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQVn3805g1cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluting both single head models"
      ],
      "metadata": {
        "id": "q11QKvRzpWSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating function. accuracy is considered as metric\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        losses = []\n",
        "\n",
        "\n",
        "        for images, targets in dataloader:\n",
        "            images = images.float()\n",
        "            targets = targets.float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "\n",
        "\n",
        "            batch_loss = 0\n",
        "            for output in outputs:\n",
        "                pred_boxes = output[0]\n",
        "\n",
        "                pred_boxes = pred_boxes.view(-1, 4)\n",
        "\n",
        "                target_size = pred_boxes.shape[0:]\n",
        "                targets_resized = F.interpolate(targets.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)\n",
        "                batch_loss += criterion(pred_boxes, targets)\n",
        "\n",
        "\n",
        "            mean_loss = batch_loss / len(outputs)\n",
        "            losses.append(mean_loss.item())\n",
        "\n",
        "            #here loss is accuracy\n",
        "            return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "appliance_test_dataset_path = '/content/appliance-dataset-5-tat-10/images/test2017'\n",
        "appliance_test_annotation_file = '/content/appliance-dataset-5-tat-10/annotations/instances_test2017.json'\n",
        "food_test_dataset_path = '/content/food-dataset-10-tat-10/images/test2017'\n",
        "food_test_annotation_file = '/content/food-dataset-10-tat-10/annotations/instances_test2017.json'\n",
        "\n",
        "# Loading the test datasets\n",
        "test_dataset_appliance = CocoDataset(appliance_test_dataset_path, appliance_test_annotation_file)\n",
        "test_dataset_food = CocoDataset(food_test_dataset_path, food_test_annotation_file)\n",
        "\n",
        "# Creating data loaders\n",
        "test_loader_appliance = torch.utils.data.DataLoader(test_dataset_appliance, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader_food = torch.utils.data.DataLoader(test_dataset_food, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Loading the saved model\n",
        "model = create_model('tf_efficientdet_d0', pretrained=False)  # Make sure to create the model instance first\n",
        "model.load_state_dict(torch.load('efficientdetappliance_model.pth'))\n",
        "\n",
        "single_head_appliance_performance = evaluate_model(model, test_loader_appliance, criterion)\n",
        "\n",
        "# Loading the saved model\n",
        "model = create_model('tf_efficientdet_d0', pretrained=False)  # Make sure to create the model instance first\n",
        "model.load_state_dict(torch.load('efficientdetFoodData_model.pth'))\n",
        "\n",
        "single_head_food_performance = evaluate_model(model, test_loader_food, criterion)\n",
        "print(\"here metric is accuracy\")\n",
        "print(f'Single Head Food Performance: {single_head_appliance_performance}')\n",
        "print(f'single Head Food Performance: {single_head_food_performance}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIz8aolthH83",
        "outputId": "eedbaec1-22e6-4133-d14c-e4702c2bb5bc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([3317760, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([147456, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "WARNING:timm.models._builder:Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here metric is accuracy\n",
            "Single Head Food Performance: 86.703678719287\n",
            "single Head Food Performance: 84.17894686721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By observing both evalution outputs\n",
        "\n",
        "\n",
        "The evalution performance using dual heads on both datasets is matches(almost same) with evalution performance using single heads on both datasets"
      ],
      "metadata": {
        "id": "g1NShDUJp4zO"
      }
    }
  ]
}